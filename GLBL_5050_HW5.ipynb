{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d70cae-fedc-4dae-be43-c311a6498c95",
   "metadata": {},
   "source": [
    "## Writing to Files and Reading from Them\n",
    "\n",
    "So far, we have been working in \"console world\", a world where we print our output to our Jupyter notebeook cells, and we are happy to do it. But that has it's limitations. What makes these techniquest powerful, is our ability to save files to be accessed later, and to grab data and files from our computers or from data on the internet, or the world wide infonet as it was once called, albeit briefly.\n",
    "\n",
    "Let's go there! \n",
    "\n",
    "### Writing to a File:\n",
    "\n",
    "#### Some basics: \n",
    "Following the \"law of primacy\" ie; what you first learn you will remember most...here is the correct format for writing to a file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b924d1d1-dc26-4e72-872b-c762694cb36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content written to example.txt successfully.\n"
     ]
    }
   ],
   "source": [
    "#Please assign 'first_lines_leader.txt'. \n",
    "#Please replace \"content\" with the first 3 lines of your favorite speech made by a world leader.\n",
    "#Then, run the code:\n",
    "\n",
    "filename = 'example.txt'\n",
    "content = 'This is the content to be written to the file.'\n",
    "with open(filename, 'w') as file:\n",
    "    file.write(content)\n",
    "print(f\"Content written to {filename} successfully.\")\n",
    "\n",
    "#Go to your finder. You should find your file in the same location as where you are working on this notebook.\n",
    "#Confirm you located your file. it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49124258-9e82-4c20-8661-411ee0b584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"first_lines_leader.txt\"\n",
    "content = \"\"\"I would like to thank the Secretary General of the United Nations for inviting me to be part of the Fourth World Conference on Women. \n",
    "\n",
    "This is truly a celebration - It is also a coming together…\n",
    "\n",
    "Our goals for this Conference, to strengthen families and societies by empowering women to take greater control over their own destinies, cannot be fully achieved unless all governments - here and around the world - accept their responsibility to protect and promote internationally recognized human rights. \"\"\"\n",
    "with open(filename, 'w') as file: \n",
    "    file.write(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b69e21-b688-4a60-866a-a8dd365be620",
   "metadata": {},
   "source": [
    "### Let's break it down:\n",
    "\n",
    "1. Filename and content as variables:\n",
    "I assigned 'example.txt' to the filename variable and the string to the content variable. This is good practice because it makes the code more flexible and easier to modify.\n",
    "\n",
    "2. with open('filename', 'w') as file:\n",
    "This opens (or creates) a file named 'filename' in write mode ('w'), which will overwrite any existing content in the file.\n",
    "file is the file object you are working with. The file is automatically closed after the block.\n",
    "\n",
    "3. The content is then written to the file object you created. By the way you can call \"file\" anything. Lot's of times I'll write \"fo\" or even f\n",
    "\n",
    "4. A print statement confirms it worked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30786fe2-816d-4bc9-8bc1-ca6bec06a18c",
   "metadata": {},
   "source": [
    "### Oh....Did you find your file in your finder? Assuming the answer is \"yes\"...good job! Now let's grab that content and read it to the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ce9319-c77d-4a29-bbd7-34538bd462b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to thank the Secretary General of the United Nations for inviting me to be part of the Fourth World Conference on Women. \n",
      "\n",
      "This is truly a celebration - It is also a coming together…\n",
      "\n",
      "Our goals for this Conference, to strengthen families and societies by empowering women to take greater control over their own destinies, cannot be fully achieved unless all governments - here and around the world - accept their responsibility to protect and promote internationally recognized human rights. \n"
     ]
    }
   ],
   "source": [
    "#Below:\n",
    "#filename is a string that holds the name of the file.\n",
    "#file is the file object created by open(filename, 'r').\n",
    "#The file.read() method reads the content of the file.\n",
    "#The print(content) prints out what was read from the file.\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    content = file.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e23c4b-ffb5-47d4-84a1-9e0ee467aa4e",
   "metadata": {},
   "source": [
    "### Read vs Readlines\n",
    "Above, we see that we can grab all the content of the fie using read(). This is a great way to go if we want to work with the file content as a whole, for example, searching for a word in the entire file, or processing word frequency distribution on a presidential speech.\n",
    "\n",
    "But what if the the file is very large? Teading the entire content into memory can be inefficient or even cause memory issues. \n",
    "\n",
    "Or what if we want to process the file line by line and pull out spaces, or other extraneous info that we don't want? \n",
    "\n",
    "Readlines to the rescue! \n",
    "Readlines reads the file line by line and returns a list of strings, where each string represents a line from the file.\n",
    "\n",
    "Use readlines() when you need to work with the file line by line, and you want to process or iterate over the lines.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6242243-2ef8-4dbf-8458-22dd4a65d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to thank the Secretary General of the United Nations for inviting me to be part of the Fourth World Conference on Women.\n",
      "\n",
      "This is truly a celebration - It is also a coming together…\n",
      "\n",
      "Our goals for this Conference, to strengthen families and societies by empowering women to take greater control over their own destinies, cannot be fully achieved unless all governments - here and around the world - accept their responsibility to protect and promote internationally recognized human rights.\n"
     ]
    }
   ],
   "source": [
    "with open('first_lines_leader.txt', 'r') as file:\n",
    "    lines = file.readlines()  # Reads the file and returns a list of lines\n",
    "for line in lines:\n",
    "    print(line.strip())  # Strip removes newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6e907-cce7-44f5-838a-9bdc3257b0a4",
   "metadata": {},
   "source": [
    "### Alternative for very large files: \n",
    "I want to introduce you to this idea, because ChatGPT might suggest it, so I want you to be exposed and know what's going on.\n",
    "For very large files, neither read() nor readlines() may be efficient. Instead, you can read the file line by line without loading the whole file into memory using what is called a file object iterator:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87507f3a-a99a-4228-92da-91afd4e1a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to thank the Secretary General of the United Nations for inviting me to be part of the Fourth World Conference on Women.\n",
      "\n",
      "This is truly a celebration - It is also a coming together…\n",
      "\n",
      "Our goals for this Conference, to strengthen families and societies by empowering women to take greater control over their own destinies, cannot be fully achieved unless all governments - here and around the world - accept their responsibility to protect and promote internationally recognized human rights.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('first_lines_leader.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1983a-5a7e-402e-8fd3-4b53c3db0f24",
   "metadata": {},
   "source": [
    "That said, you should use read() or readlines() in the majority of your cases as your file size will be moderately large at most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a1237-9306-494b-b69e-b9a0003bee44",
   "metadata": {},
   "source": [
    "### Some summary:\n",
    "\n",
    "Opening a File:\n",
    "\n",
    "Modes:\n",
    "'r' = read\n",
    "'w' = write (overwrites the file if it exists)\n",
    "'a' = append (adds to the end of the file)\n",
    "'b' = binary mode (for non-text files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d646c1-07f1-4e6a-9077-4b96dc620a66",
   "metadata": {},
   "source": [
    "## All the President's Speeches \n",
    "20 pts\n",
    "\n",
    "As we learned on Thursday, word frequency distribution is an intuitive way of determining the major themes, topics, or even preoccupations of the speech giver.\n",
    "\n",
    "Pull texts from different US presidents and use FreqDist() to gain insights into the data. This is a fairly open assignment. The goal is to identify something interesting! Pull the text, tokenize, remove stopwords, and plot the frequency of the words. That's the technical part. But much of this assignment is your ability to use word frequency to gain insights comparatively, intra or inter presidents. Let the python notebook from our lecture be your guide! The more interesting and creative the analysis, the more points you will receive. We might pick one student's analysis to be displayed in lecture, and that person will receive bonus credit for the work.\n",
    "\n",
    "For full credit, please demonstrate the use of read(), readlines(), and write() to a file in this assignment. Where you do it? Up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627fcae-fe53-4274-b15d-0096250020f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Use as many cells as you need to. Your code here:\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.data.path.clear()\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#tokenization -> way to divide up each sentence into single words \n",
    "# creating tokens, we can look at frequency distributions -> how many words of a certain type is included? \n",
    "\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "print(inaugural.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac96e3f-7df9-43ac-b44b-d633b1d1ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REFERENCES \n",
    "\n",
    "wash = inaugural.raw(\"1789-Washington.txt\")\n",
    "print(wash)\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "wash1 = inaugural.words(\"1789-Washington.txt\")\n",
    "print(wash1)\n",
    "#divides each combination of letters and symbols into tokens\n",
    "\n",
    "len(wash1)\n",
    "\n",
    "fdist = FreqDist(wash1)\n",
    "print(fdist.most_common(20))\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "print(stopwords)\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "clean_wash1 = []\n",
    "\n",
    "for word in wash1:\n",
    "    if not word in stopwords: \n",
    "        clean_wash1.append(word)\n",
    "\n",
    "fdist2 = FreqDist(clean_wash1)\n",
    "\n",
    "x = fdist2.most_common(20)\n",
    "print(x)\n",
    "\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "print(fdist2.plot(20))\n",
    "\n",
    "#data being obscured by symbols like \".\" \"-\" and \";\" \n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "new_stops = [',', \"I\", \".\", \";\", \"-\"]\n",
    "\n",
    "stopwords.extend(new_stops)\n",
    "print(stopwords)\n",
    "\n",
    "#append can only add one, extend function adds multiple \n",
    "\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "clean_wash2 = []\n",
    "for word in clean_wash1: \n",
    "    if not word in stopwords:\n",
    "        clean_wash2.append(word)\n",
    "\n",
    "fdist2 = FreqDist(clean_wash2)\n",
    "x = fdist2.most_common(20)\n",
    "print(x)\n",
    "print(fdist2.plot(20))\n",
    "    \n",
    "## --------------------------------------------\n",
    "\n",
    "# can also look at bigrams (example to look at \"united\" and \"states\")\n",
    "\n",
    "fdist_bi2 = list(nltk.bigrams(clean_wash2))\n",
    "fdist3 = FreqDist(fdist_bi2)\n",
    "x = fdist3.most_common(20)\n",
    "print(x)\n",
    "print(fdist3.plot(20))\n",
    "\n",
    "## --------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist((target, fileid[:4])\n",
    "\n",
    "                                for fileid in inaugural.fileids()\n",
    "\n",
    "                                for word in inaugural.words(fileid)\n",
    "\n",
    "                                for target in ['liberty', 'equality', 'brotherhood']\n",
    "\n",
    "                                if word.lower().startswith(target))\n",
    "\n",
    "cfd.plot(title=\"French ideals in US-American speeches through time\")\n",
    "\n",
    "## --------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f42e8d-677b-4878-a545-3b824e59a68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inaugural' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lincoln \u001b[38;5;241m=\u001b[39m inaugural\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1861-Lincoln.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m lincoln2 \u001b[38;5;241m=\u001b[39m inaugural\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1865-Lincoln.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m wilson \u001b[38;5;241m=\u001b[39m inaugural\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1917-Wilson.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inaugural' is not defined"
     ]
    }
   ],
   "source": [
    "lincoln = inaugural.words(\"1861-Lincoln.txt\")\n",
    "lincoln2 = inaugural.words(\"1865-Lincoln.txt\")\n",
    "wilson = inaugural.words(\"1917-Wilson.txt\")\n",
    "roosevelt = inaugural.words(\"1941-Roosevelt.txt\")\n",
    "roosevelt2 = inaugural.words(\"1945-Roosevelt.txt\")\n",
    "johnson = inaugural.words(\"1965-Johnson.txt\")\n",
    "truman = inaugural.words(\"1949-Truman.txt\")\n",
    "nixon = inaugural.words(\"1969-Nixon.txt\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Make sure numpy is imported\n",
    "\n",
    "new_stops = [',', \"I\", \".\", \";\", \"-\",\"?\", \"The\", \"If\", \"It\", \"--\",\"would\",'\"',\"'\", \"us\", \"We\", \":\"]\n",
    "stopwords.extend(new_stops)\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "## incorporating read(), readlines(), and write() \n",
    "\n",
    "with open(\"1861-Lincoln.txt\", 'r') as file: \n",
    "    lincoln_content = file.read()\n",
    "\n",
    "with open(\"1861-Lincoln.txt\", 'r') as file: \n",
    "    lincoln_lines = file.readlines()\n",
    "\n",
    "for line in lincoln_lines:\n",
    "    print(line.strip())\n",
    "\n",
    "with open(\"1861_Licncoln_copy.txt\", 'w') as file: \n",
    "    file.write(lincoln_content)\n",
    "\n",
    "\n",
    "\n",
    "# CLEANING SPEECHES\n",
    "def clean_speech(speech):\n",
    "    clean_speech = [word for word in speech if word not in stopwords]\n",
    "    return clean_speech\n",
    "\n",
    "clean_lincoln = clean_speech(lincoln)\n",
    "clean_lincoln2 = clean_speech(lincoln2)\n",
    "clean_wilson = clean_speech(wilson)\n",
    "clean_roosevelt = clean_speech(roosevelt)\n",
    "clean_roosevelt2 = clean_speech(roosevelt2)\n",
    "clean_johnson = clean_speech(johnson)\n",
    "clean_truman = clean_speech(truman)\n",
    "clean_nixon = clean_speech(nixon)\n",
    "\n",
    "lincoln_dist = FreqDist(clean_lincoln)\n",
    "lincoln2_dist = FreqDist(clean_lincoln2)\n",
    "wilson_dist = FreqDist(clean_wilson)\n",
    "roosevelt_dist = FreqDist(clean_roosevelt)\n",
    "roosevelt2_dist = FreqDist(clean_roosevelt2)\n",
    "johnson_dist = FreqDist(clean_johnson)\n",
    "truman_dist = FreqDist(clean_truman)\n",
    "nixon_dist = FreqDist(clean_nixon)\n",
    "\n",
    "all_speeches = clean_lincoln + clean_lincoln2 + clean_wilson + clean_roosevelt + clean_roosevelt2 + clean_johnson + clean_truman + clean_nixon\n",
    "all_speeches_dist = FreqDist(all_speeches)\n",
    "\n",
    "most_common_words = all_speeches_dist.most_common(100)\n",
    "\n",
    "print(most_common_words)\n",
    "\n",
    "target_words=[\"world\", \"peace\", \"freedom\", \"security\",\"God\", \"history\", \"hope\", \"justice\", \"equal\", \"power\", \"liberty\"]\n",
    "selected_speeches = [\n",
    "    \"1861-Lincoln.txt\", \"1865-Lincoln.txt\", \"1917-Wilson.txt\", \"1941-Roosevelt.txt\", \n",
    "    \"1945-Roosevelt.txt\", \"1965-Johnson.txt\", \"1949-Truman.txt\", \"1969-Nixon.txt\"\n",
    "]\n",
    "\n",
    "\n",
    "# List of frequency distributions for each speech\n",
    "freq_distributions = [\n",
    "    lincoln_dist, lincoln2_dist, wilson_dist, roosevelt_dist, \n",
    "    roosevelt2_dist, johnson_dist, truman_dist, nixon_dist\n",
    "]\n",
    "\n",
    "# List of speech labels\n",
    "speech_labels = [\"1861-Lincoln\", \"1865-Lincoln\", \"1917-Wilson\", \"1941-Roosevelt\", \n",
    "                 \"1945-Roosevelt\", \"1965-Johnson\", \"1949-Truman\", \"1969-Nixon\"]\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# x = np.arange(8)\n",
    "# n_words = len(target_words)\n",
    "\n",
    "# # Iterate through each target word and plot its frequency for each speech\n",
    "# for i, word in enumerate(target_words):\n",
    "#     # Calculate positions for this word's bars\n",
    "#     word_positions = x + i * 0.1\n",
    "#     # Get the frequency counts for this word in each speech\n",
    "#     word_counts = [freq[word] for freq in freq_distributions]\n",
    "#     # Plot the bar for this word\n",
    "#     plt.bar(word_positions, word_counts, width=bar_width, label=word)\n",
    "\n",
    "# # Set the x-tick positions to be in the middle of the grouped bars\n",
    "# plt.xticks(x + (n_words / 2) * bar_width, speech_labels, rotation=45)\n",
    "\n",
    "# # Adding titles and labels\n",
    "# plt.title('Frequency of Target Words in Presidential Speeches')\n",
    "# plt.xlabel('Speeches')\n",
    "# plt.ylabel('Word Frequency')\n",
    "# plt.legend(title='Words')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9114f2-82f8-46e6-8274-62289b15c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INFERENCES FROM DATA & BAR GRAPH \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"liberty\" and \"freedom\" appear most frequently in almost all the speeches. This reflects demotratic values during wartime, \n",
    "but we would need to compare this data with data from speeches in times of peace. \n",
    "\n",
    "Similarly, we can make similar inferences about the frequencies of the other words. However, more important important \n",
    "inferences could be made if we compared the frequency of these words in speeches between those given in times of war and \n",
    "times of peace. \n",
    "\n",
    "Notes: \n",
    "Perhaps we could use an average? comparing average use of these words between the 8 war-time inaugural speeches with the rest? \n",
    "Or, maybe we choose a couple of words and analyze those? \n",
    "\n",
    "Originally just added the number of uses of the selected words. Non-war time speeches overwhelmed war time speeches. \n",
    "This is most likely simply due to the fact that there were more non-war speeches than war speeches. \n",
    "Instead, let's look at average frequency. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6642ec9-491f-4f66-bcc0-20085a60e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incorporating read(), readline(), and write() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ae28c-b6fc-41d9-977f-aae3482209b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARING the frequency of words \"security\", \"hope\" \"peace\", \"liberty\", and \"freedom\" between speeches made during times of war and speeches made in times of peace \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import inaugural\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def clean_speech(speech):\n",
    "    return[word.lower() for word in speech if word.lower() not in stopwords]\n",
    "\n",
    "peace_speeches_txt = [\n",
    "    '1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', \n",
    "    '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', \n",
    "    '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', \n",
    "    '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', \n",
    "    '1853-Pierce.txt', '1857-Buchanan.txt', '1869-Grant.txt', '1873-Grant.txt', \n",
    "    '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', \n",
    "    '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', \n",
    "    '1909-Taft.txt', '1913-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', \n",
    "    '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1953-Eisenhower.txt', \n",
    "    '1957-Eisenhower.txt', '1961-Kennedy.txt', '1969-Nixon.txt', '1973-Nixon.txt', \n",
    "    '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', \n",
    "    '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', \n",
    "    '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt'\n",
    "]\n",
    "\n",
    "\n",
    "target_words = [\"world\", \"peace\", \"freedom\", \"security\", \"hope\", \"justice\", \"power\", \"america/american\"]\n",
    "\n",
    "peace_speeches = [inaugural.words(file) for file in peace_speeches_txt]\n",
    "peace_speeches_combined = [word for speech in peace_speeches for word in clean_speech(speech)]\n",
    "peace_speeches_freq = FreqDist(peace_speeches_combined)\n",
    "    \n",
    "war_speeches = [\n",
    "    inaugural.words(\"1861-Lincoln.txt\"), inaugural.words(\"1865-Lincoln.txt\"), \n",
    "    inaugural.words(\"1917-Wilson.txt\"), inaugural.words(\"1941-Roosevelt.txt\"), \n",
    "    inaugural.words(\"1945-Roosevelt.txt\"), inaugural.words(\"1965-Johnson.txt\"), \n",
    "    inaugural.words(\"1949-Truman.txt\"), inaugural.words(\"1969-Nixon.txt\")\n",
    "]\n",
    "\n",
    "war_speeches_combined = [word for speech in war_speeches for word in clean_speech(speech)]\n",
    "war_speeches_freq = FreqDist(war_speeches_combined)\n",
    "\n",
    "war_count = len(war_speeches)\n",
    "peace_count = len(peace_speeches)\n",
    "\n",
    "#combine \"america\" and \"american\" \n",
    "combined_war_frequency = war_speeches_freq[\"america\"] + war_speeches_freq[\"american\"]\n",
    "combined_peace_frequency = peace_speeches_freq[\"america\"] + peace_speeches_freq[\"american\"]\n",
    "\n",
    "#calculate the frequencies of all target words in war speeches\n",
    "war_frequencies = [\n",
    "    war_speeches_freq[\"peace\"], war_speeches_freq[\"security\"], war_speeches_freq[\"hope\"],\n",
    "    war_speeches_freq[\"freedom\"], war_speeches_freq[\"power\"], war_speeches_freq[\"justice\"],\n",
    "    war_speeches_freq[\"world\"], combined_war_frequency\n",
    "]\n",
    "\n",
    "#now frequencies in peace speeches \n",
    "peace_frequencies = [\n",
    "    peace_speeches_freq[\"peace\"], peace_speeches_freq[\"security\"], peace_speeches_freq[\"hope\"],\n",
    "    peace_speeches_freq[\"freedom\"], peace_speeches_freq[\"power\"], peace_speeches_freq[\"justice\"],\n",
    "    peace_speeches_freq[\"world\"], combined_peace_frequency\n",
    "]\n",
    "\n",
    "# Calculate average frequency of target words in wartime and peacetime speeches\n",
    "average_war_frequencies = [freq / war_count for freq in war_frequencies]\n",
    "average_peace_frequencies = [freq / peace_count for freq in peace_frequencies]\n",
    "\n",
    "# Make bar graph using the averages you just found\n",
    "\"\"\"we should prolly change the formatting of our graph because there are way too many speeches\n",
    "to fit on the x axis. so x axis would be words with two bars each, comparing average of war \n",
    "speeches and peace speeches \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "x = np.arange(len(target_words))  # Define positions for the number of target words\n",
    "\n",
    "bar_width = 0.35  # Width of each bar\n",
    "\n",
    "# Plot the bars for wartime and peacetime frequencies\n",
    "for i, word in enumerate(target_words):\n",
    "    plt.bar(x[i] - bar_width/2, average_war_frequencies[i], width=bar_width, color='b', label='Wartime' if i == 0 else \"\")\n",
    "    plt.bar(x[i] + bar_width/2, average_peace_frequencies[i], width=bar_width, color='orange', label='Peacetime' if i == 0 else \"\")\n",
    "\n",
    "# Set the x-tick positions to be in the middle of the grouped bars and label them with target words\n",
    "plt.xticks(x, target_words, rotation=45)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Average Frequency of Target Words in Wartime vs. Peacetime Speeches')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Average Frequency')\n",
    "plt.legend(title='Context')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x = np.arange(len(target_words))\n",
    "# width = 0.35\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# bars1 = ax.bar(x - width/2, average_war_frequencies, width, label='Wartime Average')\n",
    "# bars2 = ax.bar(x + width/2, average_peace_frequencies, width, label='Peacetime Average')\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f1e6d-29b5-4001-a90a-a2581a242bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##INFERENCES 2 \n",
    "\n",
    "\"\"\" World, Peace, Power, and America/American\n",
    "\n",
    "In war-time speeches, the words \"world\", \"peace\", and \"power\" are used more compared to speeches made during\n",
    "peaceful times. This suggests that during wartime, there was a focus on global contexts. The word \"power\" may\n",
    "have been used more in war-time speeches in discussions of the military or the United States as a global power\n",
    "during times of war. \n",
    "\n",
    "Contrastly, the word \"America\" or \"American\" is used much more in times of peace than in times of war. This is\n",
    "interesting to place against the word \"world\" as the focus of speeches in war-time are issues in the global \n",
    "context, while\n",
    "\n",
    "The word \"hope\" is more emphasized in peace-time speeches. This could infer that the President's giving such speeches\n",
    "had an optimistic outlook on the nation's future, which is more common when not facing conflict, violence, and war. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bafa1-7f12-4cb8-8270-69b1cb368304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23192ed-3e43-4536-9177-92d4bc81c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3cd73-d4bc-4df3-a7d5-6ff8cd598ab9",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "That's great that we can analyze text that is pre-processed in the NLTK package. But what if NLTK doesn't have the text we want? I mean as awesome as Jane Austen and inaugural texts are.... \n",
    "\n",
    "Text scraping, also known as web scraping or data scraping, refers to the process of automatically extracting text data from websites or other online sources. Over my 14 years, I have seen companies and websites realize that they should not just easily give away their text data. Scraping used to be easy. Data used to be easy to grab from the internet. It's gotten harder for some sites, but others allow access, or simply haven't gotten around to restricting it. **WHEN SCRAPING, PLEASE MAKE SURE YOU ARE NOT VIOLATING TERMS OF USE**. Usually, these apply to commercial use, and for research or academic purposes, there are fewer restrictions. \n",
    "\n",
    "Below is some code for you to scrape a website. The package is called \"Beautiful Soup\" I have given you code below to use.\n",
    "\n",
    "I have given you TWO websites to scrape. PLEASE PICK ONE of TWO. Either:\n",
    "\n",
    "A speech from Christine LaGrande on Sustainable Development Goal: Here is the url: https://www.imf.org/en/News/Articles/2018/09/17/sp09172018-the-case-for-the-sustainable-development-goals' OR\n",
    "\n",
    "\"Joint Statement of the Russian Federation and the People’s Republic of China on the International Relations Entering a New Era and the Global Sustainable Development\" Feb, 2022. Here is the url: http://www.en.kremlin.ru/supplement/5770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f523e7-2ffc-437d-9c49-4fbcb51293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "base_url = #use one the urls above as a string\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text)\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "for p in paragraphs:\n",
    "    print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053edd6-7900-40cf-9bdb-b1c5b191f358",
   "metadata": {},
   "source": [
    "5 pts Modify the text above to write the text to a list called \"document\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd516db-0d54-434a-be1e-be0f06a3cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "base_url = \"http://www.en.kremlin.ru/supplement/5770\"\n",
    "r = requests.get(base_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "document = []\n",
    "\n",
    "for p in paragraphs: \n",
    "    document.append(p.get_text())\n",
    "    \n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb9fd0-ae4b-4608-82f7-9f34d216d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "5 pts Write the code to a file called \"document.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb238d1-a64a-4a96-88d9-7d15569dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "filename = 'document.txt'\n",
    "with open(filename, 'w') as file:\n",
    "    file.writelines(document)\n",
    "print(f\"Content written to {filename} successfully.\")\n",
    "\n",
    "#Go to your finder. You should find your file in the same location as where you are working on this notebook.\n",
    "#Confirm you located your file. it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d878-807a-42be-89e2-0eeabbe47c74",
   "metadata": {},
   "source": [
    "20 pts Clean the document of any extraneous elements, remove stopwords and perform a word frequency distribution analysis on your document, showing and then plotting the top 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d7c8f-c9e4-4f20-9937-49875f2d3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Your code here:\n",
    "# from nltk.probability import FreqDist\n",
    "# from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "new_stops = [',', \"I\", \".\", \";\", \"-\",\"?\", \"The\", \"If\", \"It\", \"--\",\"would\",'\"',\"'\", \"us\", \"We\", \":\"]\n",
    "stopwords.extend(new_stops)\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "doc = \"document.txt\"\n",
    "with open(filename, 'r') as file: \n",
    "    text = file.read() \n",
    "\n",
    "words = text.split()\n",
    "cleaned_doc = [word for word in words if word not in stopwords]\n",
    "\n",
    "cleaned_doc_dist = FreqDist(cleaned_doc)\n",
    "x = cleaned_doc_dist.most_common(20)\n",
    "print(x)\n",
    "\n",
    "print(cleaned_doc_dist.plot(20))\n",
    "\n",
    "## REFERENCES \n",
    "# x = fdist2.most_common(20)\n",
    "# print(x)\n",
    "# print(fdist2.plot(20))\n",
    "\n",
    "\n",
    "# clean_wash1 = []\n",
    "\n",
    "# for word in wash1:\n",
    "#     if not word in stopwords: \n",
    "#         clean_wash1.append(word)\n",
    "\n",
    "# fdist2 = FreqDist(clean_wash1)\n",
    "\n",
    "# x = fdist2.most_common(20)\n",
    "# print(x)\n",
    "\n",
    "\n",
    "# ## --------------------------------------------\n",
    "\n",
    "# print(fdist2.plot(20))\n",
    "\n",
    "# #data being obscured by symbols like \".\" \"-\" and \";\" \n",
    "\n",
    "# ## --------------------------------------------\n",
    "\n",
    "# new_stops = [',', \"I\", \".\", \";\", \"-\"]\n",
    "\n",
    "# stopwords.extend(new_stops)\n",
    "# print(stopwords)\n",
    "\n",
    "# #append can only add one, extend function adds multiple \n",
    "\n",
    "\n",
    "# ## --------------------------------------------\n",
    "\n",
    "# clean_wash2 = []\n",
    "# for word in clean_wash1: \n",
    "#     if not word in stopwords:\n",
    "#         clean_wash2.append(word)\n",
    "\n",
    "# fdist2 = FreqDist(clean_wash2)\n",
    "# x = fdist2.most_common(20)\n",
    "# print(x)\n",
    "# print(fdist2.plot(20))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74243478-22c7-4fbc-b03e-95699c62c874",
   "metadata": {},
   "source": [
    "5 pts Please read the speech. What does the word frequency tell us, if anything, about the speech? Please answer in a short paragraph that tells me how the word frequency distribution is good at finding meaning, but also may miss the meaning that you gained from actually reading the article.\n",
    "\n",
    "Your paragraph here:\n",
    "\n",
    "The word frequency analysis does help us somewhat understand the statement, for example, the words \"sides\" (used to refer to the two states: China and Russia), \"international\", \"global\", and \"cooperation\" suggest partnership between the two nations. However, if we only using the frequency of these words to make inferences in an analysis, we may end up with inaccurate conclusions. For example, the word \"United\" is used a total of 19 times, however, we are unsure if it represents the United Nations or the United States. This could be avoided with the use of bigrams. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02613f82-9c85-4581-a5e3-b194d7feeca1",
   "metadata": {},
   "source": [
    "### Intro to Summarization\n",
    "Summarization has been a thorny problem for data scientists over the past decade. There have been many approaches, but most, until ChatGPT and it's use of the transformers' architecture, they have been unsatisfactory. There are two main types of summarization:\n",
    "\n",
    "Extractive Summarization: Finding the most important sentences and rendering them verbatim.\n",
    "\n",
    "Abstractive Summarization: Taking the most important ideas from a document and generating a summary based on them\n",
    "\n",
    "There have been many different algorithms that have attempted summarization. Let's learn about one of the most used and deploy it in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7cd69-83a1-400a-8820-2d0929b5d52d",
   "metadata": {},
   "source": [
    "#### Luhns Heuristic Method\n",
    "This is one of the earliest approaches of text summarization. This is related to our manual interpretation of the importance of word frequency in a document.Basically, just as we did, Luhn recognized that word frequency and word significance is highly correlated, but only \"important\" words. The assumption that the most important sentences are those with the most significant words. Here is a nice short article that explains the algo in an accessible way: https://blog.fastforwardlabs.com/2016/03/25/h.p.-luhn-and-the-heuristic-value-of-simplicity.html\n",
    "\n",
    "##### Sumy is a python package that has a lot of different commons summarization methods including LSA, LexRank and Luhn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937ffbd-8fc9-4eff-91bc-dfdbcc4048e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install sumy package\n",
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819dde4b-4a7a-45b6-a728-a93d7fc8ce06",
   "metadata": {},
   "source": [
    "Run the following code. Pass the document you want to summarize as a variable in the document part of the statement below. Use the variable that you saved the document into if it's something different.\n",
    "\n",
    "PlaintextParser.from_string(document,Tokenizer(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d00b0b-10d6-49ea-9c27-24af0c3f3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "parser = PlaintextParser.from_string(document,Tokenizer(\"english\"))\n",
    "\n",
    "document = \"document.txt\"\n",
    "\n",
    "summarizer_luhn = LuhnSummarizer()\n",
    "summary_1 =summarizer_luhn(parser.document,2)\n",
    "for sentence in summary_1:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1612c2-f4fb-4b53-b851-876a4176ce42",
   "metadata": {},
   "source": [
    "10 pts Evaluate the Luhn summarization below on the document you selected in the previous problem. How did it do?\n",
    "\n",
    "Your evaluation here: The basic themes are reflected in the Luhn summary, however, there are important parts of the statement including specfics/details that are missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781fdfc-13fe-4b5b-8c3d-a827304e86b5",
   "metadata": {},
   "source": [
    "10 pts Now look at the documentation for Sumy and use the Lex Rank summarization algo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de74fde-eb5e-459f-b041-632f8da2c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "parser = PlaintextParser.from_string(document,Tokenizer(\"english\"))\n",
    "\n",
    "with open(\"document.txt\", \"r\") as file: \n",
    "    document = file.read()\n",
    "\n",
    "summarizer_lexrank = LexRankSummarizer()\n",
    "summary_lexrank =summarizer_lexrank(parser.document,2)\n",
    "for sentence in summary_lexrank:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d55601-95b4-4f97-8014-a22e0524cd11",
   "metadata": {},
   "source": [
    "How does Lex Rank work? Explain it to me and evaluate the differences in the two algos.\n",
    "\n",
    "5 pts Your explanation and evaluation here:\n",
    "\n",
    "Lex Rank compares sentences with eachother to determine how similar they are. The more similar or connected sentences are, the more it is deemed important. I imagine the Lex Rank process to use venn diagrams and analysis is based on union spaces, while the Luhn summarization process more so reflects the large-lecture in-class polls that show poll results by enlarging certain words that are more frequently submitted.\n",
    "\n",
    "Analysis of Lex Rank summary: The Lex Rank Summary provided a more detailed summary and used full sentences that I recognized from reading the straight statement. Compared to the Lex Rank Summary, the Luhn summarization provided more wholistic statements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad17b90-a828-4306-a725-636acfefb776",
   "metadata": {},
   "source": [
    "#### Using ChatGPT for Summarization\n",
    "Now use Chatgpt to perform extractive and abstractive summarization on your document. Copy and paste into prompt. You will see that the document may be too long, so you may have to break the document into parts. Does where you break your document impact the summarization?\n",
    "\n",
    "5 pts Copy and paste your prompt and the ChatGPT extractive summarization here:\n",
    "\n",
    "\n",
    "The document discusses a joint statement by Russia and China, emphasizing the importance of international cooperation, democracy, and mutual respect. They advocate for a multipolar world order, criticize unilateral actions by certain states, and oppose the imposition of external democratic standards. The two countries reaffirm their commitment to international security, disarmament, and sustainable development, and call for global efforts to combat challenges such as terrorism, climate change, and pandemics. They also highlight cooperation in economic development and technological innovation through multilateral platforms like the UN, BRICS, and SCO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da82e-7aeb-48b8-8a82-a6455b41232e",
   "metadata": {},
   "source": [
    "5 pts Copy and paste your prompt and the ChatGPT abstractive summarization here:\n",
    "\n",
    "The document is a joint statement between Russia and China emphasizing their commitment to strengthening international cooperation, upholding principles of sovereignty, and promoting multipolarity. They call for greater global collaboration on security, economic development, human rights, and environmental issues, while criticizing unilateral actions by certain states that disrupt international stability. They highlight the need for a new type of international relations based on mutual respect, peaceful coexistence, and rejecting hegemonic practices that threaten global peace and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c850c-bf83-412c-b1ee-ee48e6b80550",
   "metadata": {},
   "source": [
    "5 pts Does breaking the document in different places impact the summarization? Answer here:\n",
    "\n",
    "Breaking the document in different places does impact summarization as some sentences may be deemed more important in the midst of some sentences than in the midst of other sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338a145-bd59-4039-a7e8-5de9d14f0b73",
   "metadata": {},
   "source": [
    "5 pts Watch this video, and be prepared for a quiz on it! \n",
    "https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba46f0-ce05-40d4-a1f9-1bb99d993925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
